{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e627a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:13.222811Z",
     "start_time": "2023-08-13T21:24:12.982699Z"
    }
   },
   "outputs": [],
   "source": [
    "from pddl.logic import Predicate, constants, variables\n",
    "from pddl.core import Domain, Problem, Action, Requirements\n",
    "from pddl.formatter import domain_to_string, problem_to_string\n",
    "from pddl import parse_domain, parse_problem\n",
    "\n",
    "from pddl.logic.effects import When, AndEffect\n",
    "from pddl.logic.base import Not, And\n",
    "from pddl.logic.predicates import EqualTo\n",
    "from pddl.custom_types import name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "import sys, os, csv\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc94b63",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf7f72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:13.248677Z",
     "start_time": "2023-08-13T21:24:13.237358Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_folder_names(path):\n",
    "    folder_names = []\n",
    "    for root, _, _ in os.walk(path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        if folder_name.startswith('p'):\n",
    "            folder_names.append(folder_name)\n",
    "    folder_names = sorted(list(set(folder_names)))\n",
    "    return folder_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d0a67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:13.279407Z",
     "start_time": "2023-08-13T21:24:13.262699Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def extract_random_subset(input_filename):\n",
    "    # Read the input file\n",
    "    with open(input_filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Determine the maximum number of lines to be extracted\n",
    "    max_lines = min(5, len(lines))\n",
    "    \n",
    "    # Generate a random number between 0 and this maximum\n",
    "    num_lines = random.randint(0, max_lines)\n",
    "\n",
    "    # Extract the specified number of lines\n",
    "    subset = lines[:num_lines]\n",
    "    if subset and subset[-1].endswith(\"\\n\"):\n",
    "            subset[-1] = subset[-1].rstrip(\"\\n\")\n",
    "    # Derive the output filename\n",
    "    base, ext = os.path.splitext(input_filename)\n",
    "    output_filename = f\"{base}_upto{ext}\"\n",
    "\n",
    "    # Save these lines to the output file\n",
    "    with open(output_filename, 'w') as f:\n",
    "        f.writelines(subset)\n",
    "        \n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94892c72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:13.283747Z",
     "start_time": "2023-08-13T21:24:13.280799Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_files(file1_path, file2_path, output_path):\n",
    "    merged_content = []\n",
    "\n",
    "    # Read content from the first file and append to the merged content\n",
    "    with open(file1_path, 'r') as f1:\n",
    "        for line in f1:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(\";\"):\n",
    "                merged_content.append(line + '\\n')\n",
    "\n",
    "    # Read content from the second file, filter out lines starting with ';' or empty lines, and append to the merged content\n",
    "    with open(file2_path, 'r') as f2:\n",
    "        for line in f2:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(\";\"):\n",
    "                merged_content.append(line + '\\n')\n",
    "    if merged_content and merged_content[-1].endswith(\"\\n\"):\n",
    "            merged_content[-1] = merged_content[-1].rstrip(\"\\n\")\n",
    "    # Write the merged content to the output file\n",
    "    with open(output_path, 'w') as out_file:\n",
    "        out_file.writelines(merged_content)\n",
    "\n",
    "    print(f\"Merged content saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4cd5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:13.288343Z",
     "start_time": "2023-08-13T21:24:13.285518Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prob_template_for_hyps (domain_folder_path, problem_file):    \n",
    "    problem_file = parse_problem ( problem_file)\n",
    "    problem_file_string = problem_to_string(problem_file)\n",
    "    # Find the position of '(:goal'\n",
    "    start_pos = problem_file_string.find('(:goal')\n",
    "\n",
    "    # Check if '(:goal' was found\n",
    "    if start_pos != -1:\n",
    "        # Replace everything after '(:goal' with '<HYPOTHESIS>)))'\n",
    "        problem_file_string = problem_file_string[:start_pos] + '(:goal \\n(and \\n \\t<HYPOTHESIS> \\n)))'\n",
    "\n",
    "    with open(f\"{domain_folder_path}/template.pddl\", 'w') as file:\n",
    "        file.write(problem_file_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05afd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:13.292479Z",
     "start_time": "2023-08-13T21:24:13.289285Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_obs_dat_file(sas_plan_file_path, obs_file_path):\n",
    "    with open(sas_plan_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Get rid of last line which is cost\n",
    "    lines = lines[:-1]\n",
    "\n",
    "    print(len(lines))\n",
    "    \n",
    "\n",
    "    num_lines_to_keep = len(lines)\n",
    "\n",
    "\n",
    "    # Get the first one-third of the lines\n",
    "    first_one_third_lines = lines[:num_lines_to_keep]\n",
    "    \n",
    "    # Check if the last line of first_one_third_lines ends with '\\n'\n",
    "    # If it does, remove the '\\n'\n",
    "    if first_one_third_lines and first_one_third_lines[-1].endswith('\\n'):\n",
    "        first_one_third_lines[-1] = first_one_third_lines[-1].rstrip('\\n')\n",
    "\n",
    "    # Save the first one-third of the lines to the new file\n",
    "    with open(obs_file_path, 'w') as new_file:\n",
    "        new_file.writelines(first_one_third_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaab431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:13.295115Z",
     "start_time": "2023-08-13T21:24:13.293277Z"
    }
   },
   "outputs": [],
   "source": [
    "directory_path_1 = 'xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a0e07b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:13.298872Z",
     "start_time": "2023-08-13T21:24:13.295996Z"
    }
   },
   "outputs": [],
   "source": [
    "domain_path_list = [directory_path_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c5dda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:13.306013Z",
     "start_time": "2023-08-13T21:24:13.299854Z"
    }
   },
   "outputs": [],
   "source": [
    "folders = get_all_folder_names(directory_path_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bcf06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T23:02:52.760367Z",
     "start_time": "2023-08-13T23:02:52.754268Z"
    }
   },
   "outputs": [],
   "source": [
    "folders[5:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040452b",
   "metadata": {},
   "source": [
    "# Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2180bcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T23:03:05.880666Z",
     "start_time": "2023-08-13T23:03:05.655570Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name_list_after_t_merger = ['merged_domain_with_constants.pddl','merged_problem_for_obs.pddl', 'merged_domain.pddl', 'merged_problem.pddl','hyps.dat']\n",
    "\n",
    "for a_domain_path in domain_path_list:\n",
    "    problem_instance_folder_list = get_all_folder_names(a_domain_path)\n",
    "    \n",
    "    for a_problem_instance in problem_instance_folder_list[5:6]:\n",
    "        \n",
    "        print(a_problem_instance)\n",
    "        merger_path = f'{a_domain_path}/t_merger.py'\n",
    "        human_domain = f'{a_domain_path}/{a_problem_instance}/human_domain.pddl'\n",
    "        human_problem = f'{a_domain_path}/{a_problem_instance}/robot_problem.pddl'\n",
    "        robot_domain = f'{a_domain_path}/{a_problem_instance}/robot_domain.pddl'\n",
    "        \n",
    "        os.system(f\"python {merger_path} -h_d {human_domain} -h_i {human_problem} -r_d {robot_domain} \")\n",
    "\n",
    "        current_directory = os.getcwd()\n",
    "        \n",
    "        for a_merged_file in file_name_list_after_t_merger:\n",
    "            source_file_path = f'{current_directory}/{a_merged_file}'\n",
    "            destination_file_path = f'{a_domain_path}/{a_problem_instance}/{a_merged_file}'\n",
    "            \n",
    "            os.rename(source_file_path, destination_file_path)\n",
    "        \n",
    "        get_prob_template_for_hyps(f'{a_domain_path}/{a_problem_instance}', f\"{a_domain_path}/{a_problem_instance}/merged_problem.pddl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb0cc14",
   "metadata": {},
   "source": [
    "## Get observation\n",
    "From human_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6de2cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T23:04:07.525139Z",
     "start_time": "2023-08-13T23:04:07.190186Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "for a_domain_path in domain_path_list:\n",
    "    problem_instance_folder_list = get_all_folder_names(a_domain_path)\n",
    "    \n",
    "    for a_problem_instance in problem_instance_folder_list[5:6]:\n",
    "        \n",
    "        print(a_problem_instance)\n",
    "        problem_file_updater = f'{a_domain_path}/t_upd_prob_file_with_obs_list_not_merged.py'\n",
    "        \n",
    "        print('a_problem_instance:', a_problem_instance)\n",
    "        \n",
    "        \n",
    "        human_domain = f'{a_domain_path}/{a_problem_instance}/human_domain.pddl'\n",
    "        init_human_problem = f'{a_domain_path}/{a_problem_instance}/robot_problem.pddl'\n",
    "        \n",
    "        \n",
    "        non_fail_merged_robot_problem = f'{a_domain_path}/{a_problem_instance}/merged_problem_for_obs.pddl'\n",
    "        merged_domain = f'{a_domain_path}/{a_problem_instance}/merged_domain.pddl'\n",
    "\n",
    "        FD_PATH = \"/Users/xyz/Documents/downward-main_2/fast-downward.py\"\n",
    "        rest = \"--search 'lazy_greedy([ff()])'\"\n",
    "\n",
    "        os.system(f\"{FD_PATH} {merged_domain} {non_fail_merged_robot_problem} {rest} \")\n",
    "        \n",
    "        current_directory = os.getcwd()\n",
    "\n",
    "        \n",
    "        #1) Run merged domain where robot cant fail to get initial plan\n",
    "        sas_plan_path = f'{current_directory}/sas_plan'\n",
    "        obs_file_path = f'{a_domain_path}/{a_problem_instance}/obs_robot_not_failed.dat'\n",
    "        \n",
    "        #get input plan and outputs to the file path\n",
    "        create_obs_dat_file(sas_plan_path, obs_file_path)\n",
    "\n",
    "        #2) get up to 5 random non failure observations (it cna be 0 also)\n",
    "        output_obs =  extract_random_subset(obs_file_path)\n",
    "        \n",
    "        #3) Iterate the problem file upto that moment\n",
    "        os.system(f\"python {problem_file_updater} -d {human_domain} -i {init_human_problem} -o {output_obs}\")        \n",
    "        #this is j+1 bc we'll use this for the next step, current problem has been used already\n",
    "        os.rename('temp_iterated_problem.pddl',f'temp_iterated_problem_human.pddl')\n",
    "\n",
    "        \n",
    "        #4) Run human domain with updated file to get failure observation to merge with non failures\n",
    "        \n",
    "        \n",
    "        \n",
    "        os.system(f\"{FD_PATH} {human_domain} temp_iterated_problem_human.pddl {rest} \")\n",
    "        \n",
    "        #first input, second input, output path\n",
    "        merge_files(output_obs,'sas_plan',f'{a_domain_path}/{a_problem_instance}/obs.dat' )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#         os.remove('sas_plan')\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400a019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T21:24:31.905976Z",
     "start_time": "2023-08-13T21:24:31.774922Z"
    }
   },
   "outputs": [],
   "source": [
    "Valid_PATH = \"/Users/xyz/Documents/VAL-master_2/build/macos64/Release/Val--Darwin/bin/./Validate\"\n",
    "\n",
    "\n",
    "for a_domain_path in domain_path_list:\n",
    "    problem_instance_folder_list = get_all_folder_names(a_domain_path)\n",
    "    \n",
    "    for a_problem_instance in problem_instance_folder_list:\n",
    "        print('a_problem_instance:', a_problem_instance)\n",
    "\n",
    "\n",
    "\n",
    "        human_problem = f'{a_domain_path}/{a_problem_instance}/robot_problem.pddl'\n",
    "        human_domain = f'{a_domain_path}/{a_problem_instance}/human_domain.pddl'\n",
    "\n",
    "\n",
    "        plan_file = f'{a_domain_path}/{a_problem_instance}/obs.dat'\n",
    "\n",
    "\n",
    "        os.system(f\"{Valid_PATH} {human_domain} {human_problem} {plan_file}\" )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed8dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
